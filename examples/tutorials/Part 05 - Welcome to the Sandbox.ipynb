{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Welcome to the Sandbox\n",
    "\n",
    "In the last tutorials, we've been initializing our hook and all of our workers by hand every time. This can be a bit annoying when you're just playing around / learning about the interfaces. So, from here on out we'll be creating all these same variables using a special convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyError",
     "evalue": "The sklearn dependency is not installed. If you intend to use it, please install it at your command line with `pip install scikit-learn`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-387b1e9f6ed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msyft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_sandbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/syft/sandbox.py\u001b[0m in \u001b[0;36mcreate_sandbox\u001b[0;34m(gbs, verbose, download_data)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdownload_data\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sklearn\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDependencyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sklearn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scikit-learn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyError\u001b[0m: The sklearn dependency is not installed. If you intend to use it, please install it at your command line with `pip install scikit-learn`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "sy.create_sandbox(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit-learn-0.23.2.tar.gz (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.19.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.5.3)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: scikit-learn\n",
      "  Building wheel for scikit-learn (PEP 517) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status -11:\n",
      "   command: /usr/local/bin/python /usr/local/lib/python3.7/site-packages/pip/_vendor/pep517/_in_process.py build_wheel /tmp/tmplcl10sj3\n",
      "       cwd: /tmp/pip-install-hqw02p6a/scikit-learn\n",
      "  Complete output (114 lines):\n",
      "  C compiler: gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC\n",
      "  \n",
      "  compile options: '-c'\n",
      "  gcc: test_program.c\n",
      "  gcc -pthread objects/test_program.o -o test_program\n",
      "  C compiler: gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC\n",
      "  \n",
      "  compile options: '-c'\n",
      "  extra options: '-fopenmp'\n",
      "  gcc: test_program.c\n",
      "  gcc -pthread objects/test_program.o -o test_program -fopenmp\n",
      "  Compiling sklearn/__check_build/_check_build.pyx because it changed.\n",
      "  Compiling sklearn/preprocessing/_csr_polynomial_expansion.pyx because it changed.\n",
      "  Compiling sklearn/cluster/_dbscan_inner.pyx because it changed.\n",
      "  Compiling sklearn/cluster/_hierarchical_fast.pyx because it changed.\n",
      "  Compiling sklearn/cluster/_k_means_fast.pyx because it changed.\n",
      "  Compiling sklearn/cluster/_k_means_lloyd.pyx because it changed.\n",
      "  Compiling sklearn/cluster/_k_means_elkan.pyx because it changed.\n",
      "  Compiling sklearn/datasets/_svmlight_format_fast.pyx because it changed.\n",
      "  Compiling sklearn/decomposition/_online_lda_fast.pyx because it changed.\n",
      "  Compiling sklearn/decomposition/_cdnmf_fast.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_gradient_boosting.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/histogram.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/splitting.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/_binning.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/_loss.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/common.pyx because it changed.\n",
      "  Compiling sklearn/ensemble/_hist_gradient_boosting/utils.pyx because it changed.\n",
      "  Compiling sklearn/feature_extraction/_hashing_fast.pyx because it changed.\n",
      "  Compiling sklearn/manifold/_utils.pyx because it changed.\n",
      "  Compiling sklearn/manifold/_barnes_hut_tsne.pyx because it changed.\n",
      "  Compiling sklearn/metrics/cluster/_expected_mutual_info_fast.pyx because it changed.\n",
      "  Compiling sklearn/metrics/_pairwise_fast.pyx because it changed.\n",
      "  Compiling sklearn/neighbors/_ball_tree.pyx because it changed.\n",
      "  Compiling sklearn/neighbors/_kd_tree.pyx because it changed.\n",
      "  Compiling sklearn/neighbors/_dist_metrics.pyx because it changed.\n",
      "  Compiling sklearn/neighbors/_typedefs.pyx because it changed.\n",
      "  Compiling sklearn/neighbors/_quad_tree.pyx because it changed.\n",
      "  Compiling sklearn/tree/_tree.pyx because it changed.\n",
      "  Compiling sklearn/tree/_splitter.pyx because it changed.\n",
      "  Compiling sklearn/tree/_criterion.pyx because it changed.\n",
      "  Compiling sklearn/tree/_utils.pyx because it changed.\n",
      "  Compiling sklearn/utils/sparsefuncs_fast.pyx because it changed.\n",
      "  Compiling sklearn/utils/_cython_blas.pyx because it changed.\n",
      "  Compiling sklearn/utils/arrayfuncs.pyx because it changed.\n",
      "  Compiling sklearn/utils/murmurhash.pyx because it changed.\n",
      "  Compiling sklearn/utils/graph_shortest_path.pyx because it changed.\n",
      "  Compiling sklearn/utils/_fast_dict.pyx because it changed.\n",
      "  Compiling sklearn/utils/_openmp_helpers.pyx because it changed.\n",
      "  Compiling sklearn/utils/_seq_dataset.pyx because it changed.\n",
      "  Compiling sklearn/utils/_weight_vector.pyx because it changed.\n",
      "  Compiling sklearn/utils/_random.pyx because it changed.\n",
      "  Compiling sklearn/utils/_logistic_sigmoid.pyx because it changed.\n",
      "  Compiling sklearn/svm/_libsvm.pyx because it changed.\n",
      "  Compiling sklearn/svm/_liblinear.pyx because it changed.\n",
      "  Compiling sklearn/svm/_libsvm_sparse.pyx because it changed.\n",
      "  Compiling sklearn/linear_model/_cd_fast.pyx because it changed.\n",
      "  Compiling sklearn/linear_model/_sgd_fast.pyx because it changed.\n",
      "  Compiling sklearn/linear_model/_sag_fast.pyx because it changed.\n",
      "  Compiling sklearn/_isotonic.pyx because it changed.\n",
      "  Partial import of sklearn during the build process.\n",
      "  [ 1/51] Cythonizing sklearn/__check_build/_check_build.pyx\n",
      "  [ 2/51] Cythonizing sklearn/_isotonic.pyx\n",
      "  [ 3/51] Cythonizing sklearn/cluster/_dbscan_inner.pyx\n",
      "  [ 4/51] Cythonizing sklearn/cluster/_hierarchical_fast.pyx\n",
      "  [ 5/51] Cythonizing sklearn/cluster/_k_means_elkan.pyx\n",
      "  [ 6/51] Cythonizing sklearn/cluster/_k_means_fast.pyx\n",
      "  [ 7/51] Cythonizing sklearn/cluster/_k_means_lloyd.pyx\n",
      "  [ 8/51] Cythonizing sklearn/datasets/_svmlight_format_fast.pyx\n",
      "  [ 9/51] Cythonizing sklearn/decomposition/_cdnmf_fast.pyx\n",
      "  [10/51] Cythonizing sklearn/decomposition/_online_lda_fast.pyx\n",
      "  [11/51] Cythonizing sklearn/ensemble/_gradient_boosting.pyx\n",
      "  [12/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_binning.pyx\n",
      "  [13/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_gradient_boosting.pyx\n",
      "  [14/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_loss.pyx\n",
      "  [15/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/_predictor.pyx\n",
      "  [16/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/common.pyx\n",
      "  [17/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/histogram.pyx\n",
      "  [18/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/splitting.pyx\n",
      "  [19/51] Cythonizing sklearn/ensemble/_hist_gradient_boosting/utils.pyx\n",
      "  [20/51] Cythonizing sklearn/feature_extraction/_hashing_fast.pyx\n",
      "  [21/51] Cythonizing sklearn/linear_model/_cd_fast.pyx\n",
      "  [22/51] Cythonizing sklearn/linear_model/_sag_fast.pyx\n",
      "  [23/51] Cythonizing sklearn/linear_model/_sgd_fast.pyx\n",
      "  [24/51] Cythonizing sklearn/manifold/_barnes_hut_tsne.pyx\n",
      "  [25/51] Cythonizing sklearn/manifold/_utils.pyx\n",
      "  [26/51] Cythonizing sklearn/metrics/_pairwise_fast.pyx\n",
      "  [27/51] Cythonizing sklearn/metrics/cluster/_expected_mutual_info_fast.pyx\n",
      "  [28/51] Cythonizing sklearn/neighbors/_ball_tree.pyx\n",
      "  [29/51] Cythonizing sklearn/neighbors/_dist_metrics.pyx\n",
      "  [30/51] Cythonizing sklearn/neighbors/_kd_tree.pyx\n",
      "  [31/51] Cythonizing sklearn/neighbors/_quad_tree.pyx\n",
      "  [32/51] Cythonizing sklearn/neighbors/_typedefs.pyx\n",
      "  [33/51] Cythonizing sklearn/preprocessing/_csr_polynomial_expansion.pyx\n",
      "  [34/51] Cythonizing sklearn/svm/_liblinear.pyx\n",
      "  [35/51] Cythonizing sklearn/svm/_libsvm.pyx\n",
      "  [36/51] Cythonizing sklearn/svm/_libsvm_sparse.pyx\n",
      "  [37/51] Cythonizing sklearn/tree/_criterion.pyx\n",
      "  [38/51] Cythonizing sklearn/tree/_splitter.pyx\n",
      "  [39/51] Cythonizing sklearn/tree/_tree.pyx\n",
      "  [40/51] Cythonizing sklearn/tree/_utils.pyx\n",
      "  [41/51] Cythonizing sklearn/utils/_cython_blas.pyx\n",
      "  [42/51] Cythonizing sklearn/utils/_fast_dict.pyx\n",
      "  [43/51] Cythonizing sklearn/utils/_logistic_sigmoid.pyx\n",
      "  [44/51] Cythonizing sklearn/utils/_openmp_helpers.pyx\n",
      "  [45/51] Cythonizing sklearn/utils/_random.pyx\n",
      "  [46/51] Cythonizing sklearn/utils/_seq_dataset.pyx\n",
      "  [47/51] Cythonizing sklearn/utils/_weight_vector.pyx\n",
      "  [48/51] Cythonizing sklearn/utils/arrayfuncs.pyx\n",
      "  [49/51] Cythonizing sklearn/utils/graph_shortest_path.pyx\n",
      "  [50/51] Cythonizing sklearn/utils/murmurhash.pyx\n",
      "  [51/51] Cythonizing sklearn/utils/sparsefuncs_fast.pyx\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\n",
      "\u001b[?25hFailed to build scikit-learn\n",
      "\u001b[31mERROR: Could not build wheels for scikit-learn which use PEP 517 and cannot be installed directly\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\r\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the sandbox give us?\n",
    "\n",
    "As you can see above, we created several virtual workers and loaded in lots of test dataset, distributing them around the various workers so that we can practice using privacy preserving techniques such as Federated Learning.\n",
    "\n",
    "We created six workers...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also populated lots of global variables which we can use right away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the pre-populated datasets on a given worker by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Worker Search Functionality\n",
    "\n",
    "One important aspect of doing remote data science is that we want the ability to search for datasets on a remote machine. Think of a research lab wanting to query hospitals for maybe \"radio\" datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5]).tag(\"#radio\", \"#hospital1\").describe(\"The input datapoints to the hospital1 dataset.\")\n",
    "y = torch.tensor([5,4,3,2,1]).tag(\"#radio\", \"#hospital2\").describe(\"The input datapoints to the hospital2 dataset.\")\n",
    "z = torch.tensor([1,2,3,4,5]).tag(\"#fun\", \"#mnist\",).describe(\"The images in the MNIST training dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.send(bob)\n",
    "y = y.send(bob)\n",
    "z = z.send(bob)\n",
    "\n",
    "# this searches for exact match within a tag or within the description\n",
    "results = bob.search([\"#radio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can also search for datasets that are pre-populated on the sandbox workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_housing_results = bob.search([\"#boston\", \"#housing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_housing_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Virtual Grid\n",
    "\n",
    "A Grid is simply a collection of workers which gives you some convenience functions for when you want to put together a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sy.PrivateGridNetwork(*workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid.search(\"#boston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = grid.search(\"#boston\",\"#data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_target = grid.search(\"#boston\",\"#target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
